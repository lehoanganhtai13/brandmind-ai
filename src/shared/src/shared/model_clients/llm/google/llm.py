"""
Google Gemini client that follows the same design pattern as BaseLLM.
This client provides a unified interface for synchronous and asynchronous
completions, including streaming capabilities. It supports Google Search
grounding, thinking budgets, and system instructions for persistent context.
This implementation uses the Google Generative AI unified SDK, which
provides a rich set of features and robust error handling.

Requires: pip install --upgrade google-genai>=1.18
"""

import traceback
from typing import Any, Dict, List, Optional

from google import genai
from google.genai import types
from google.genai.errors import APIError
from loguru import logger

from shared.model_clients.llm.base_class import (
    CompletionResponse,
    CompletionResponseAsyncGen,
    CompletionResponseGen,
)
from shared.model_clients.llm.base_llm import BaseLLM
from shared.model_clients.llm.exceptions import CallServerLLMError
from shared.model_clients.llm.google.config import GoogleAIClientLLMConfig, SchemaLike


class GoogleAIClientLLM(BaseLLM):
    """
    An LLM client that implements the BaseLLM interface for Google's Generative AI
    (Gemini) models.

    This class provides a standardized, simplified, and robust way to perform
    synchronous and asynchronous completions, including streaming.

    Features:
    - Sync/async completion with streaming support
    - Google Search grounding for up-to-date information
    - Thinking budgets for enhanced reasoning
    - System instructions for persistent context
    - Enterprise-grade error handling and client management

    Attributes:
        config (GoogleAIClientLLMConfig): The configuration object for the client.
        _client (genai.Client): The shared Google Generative AI client instance.
        _tools (List[types.Tool]): List of tools (e.g., Google Search) available to
            the model.
    """

    def __init__(self, config: GoogleAIClientLLMConfig, **kwargs):
        self.config: GoogleAIClientLLMConfig
        super().__init__(config, **kwargs)

    def _initialize_llm(self, **kwargs) -> None:
        """
        Initializes the Google Generative AI client and configures tools.

        This method is called once during instantiation to set up the client
        for reuse, which is more efficient than creating clients on each call.
        It also configures optional tools like Google Search grounding.

        Raises:
            CallServerLLMError: If client initialization fails.
        """
        try:
            # Create one shared client for both sync and async operations
            self._client = genai.Client(api_key=self.config.api_key)
        except Exception as e:
            raise CallServerLLMError(
                f"Failed to initialize Google Gen-AI client: {e}"
            ) from e

        # Configure optional Google Search tool for grounding
        self._tools: List[types.Tool] = []
        if self.config.use_grounding:
            self._tools.append(types.Tool(google_search=types.GoogleSearch()))

        if self.config.tools:
            # If tools are provided, ensure they are compatible with the Google
            # Gen-AI client
            if isinstance(self.config.tools, list):
                self._tools.extend(self.config.tools)
            else:
                raise CallServerLLMError("Tools must be a list of functions.")

    @staticmethod
    def _extract_text(response: types.GenerateContentResponse) -> str:
        """
        Concatenate **only** text parts from the first candidate.

        This avoids calling ``response.text`` (which raises the
        *Warning: there are non-text parts in the response* message when
        `thought_signature` or other binary parts are present).

        Args:
            response: Raw response object from googleâ€‘genai.

        Returns:
            str: Combined textual content.
        """
        parts: List[Any] = []
        try:
            if response.candidates and response.candidates[0].content:
                parts = response.candidates[0].content.parts or []  # type: ignore[assignment]
                final_text = "".join(p.text for p in parts if p.text)  # type: ignore[misc]
            else:
                final_text = ""
        except Exception:
            logger.error(f"Error extracting text from response: {parts}")
            logger.error(traceback.format_exc())

            # Fallback to response.text if extraction fails
            final_text = response.text or ""
            logger.info(
                f"Using `response.text` as fallback due to error - text: {final_text}"
            )

        return final_text

    def _make_generate_config(
        self,
        *,
        system_instruction: Optional[str] = None,
        thinking_budget: Optional[int] = None,
        thinking_level: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        response_mime_type: Optional[str] = None,
        response_schema: Optional[SchemaLike] = None,
    ) -> types.GenerateContentConfig:
        """
        Build GenerateContentConfig with all necessary parameters.

        This method centralizes configuration creation, similar to how
        OpenAI client prepares messages. It handles default values and
        optional features like thinking budgets.

        Args:
            system_instruction (str, optional): Override default system instruction.
            thinking_budget (int, optional): Override thinking budget (2.5 models).
            thinking_level (str, optional): Override thinking level (Gemini 3).
            temperature (float, optional): Override default temperature.
            top_p (float, optional): Override default top_p.
            max_tokens (int, optional): Override default max tokens.
            response_mime_type (str, optional): MIME type for the response.
            response_schema (SchemaLike, optional): Schema for response validation.

        Returns:
            types.GenerateContentConfig: Complete configuration for the API call.
        """
        config_kwargs: Dict[str, Any] = {
            "temperature": temperature or self.config.temperature,
            "top_p": top_p or self.config.top_p,
            "max_output_tokens": max_tokens or self.config.max_tokens,
            "system_instruction": system_instruction or self.config.system_instruction,
            "tools": self._tools or None,
            "response_mime_type": response_mime_type or self.config.response_mime_type,
            "response_schema": response_schema or self.config.response_schema,
            "http_options": types.HttpOptions(timeout=60_000),  # 60 seconds timeout
        }

        # Configure thinking based on model version
        # Gemini 3 uses thinking_level (str), Gemini 2.5 uses thinking_budget (int)
        is_gemini_3 = "gemini-3" in self.config.model

        thinking_level_to_use = (
            thinking_level if thinking_level is not None else self.config.thinking_level
        )
        thinking_budget_to_use = (
            thinking_budget
            if thinking_budget is not None
            else self.config.thinking_budget
        )

        if is_gemini_3 and thinking_level_to_use is not None:
            # Gemini 3: use thinking_level (low, medium, high, minimal)
            level_enum = types.ThinkingLevel(thinking_level_to_use.upper())
            config_kwargs["thinking_config"] = types.ThinkingConfig(
                thinking_level=level_enum,
                include_thoughts=False,
            )
        elif not is_gemini_3 and thinking_budget_to_use is not None:
            # Gemini 2.5: use thinking_budget (integer)
            config_kwargs["thinking_config"] = types.ThinkingConfig(
                thinking_budget=thinking_budget_to_use,
                include_thoughts=False,
            )

        return types.GenerateContentConfig(**config_kwargs)

    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """
        Generates a single, non-streaming completion response synchronously.

        Args:
            prompt (str): The user input prompt to process.
            **kwargs: Additional parameters to override config defaults.

        Returns:
            CompletionResponse: The model's response with generated text.

        Raises:
            CallServerLLMError: If the API call fails.
        """
        config = self._make_generate_config(**kwargs)
        try:
            response = self._client.models.generate_content(
                model=self.config.model,
                contents=prompt,
                config=config,
            )

            text = self._extract_text(response)
            if not text:
                raise CallServerLLMError(
                    "Google Gen-AI API returned empty response text."
                )

            return CompletionResponse(text=text)
        except Exception as e:
            raise CallServerLLMError(f"Google Gen-AI API call failed: {e}")

    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        """
        Generates a streaming completion response synchronously.

        This method yields incremental responses as they become available,
        allowing for real-time display of the model's output.

        Args:
            prompt (str): The user input prompt to process.
            **kwargs: Additional parameters to override config defaults.

        Yields:
            CompletionResponse: Incremental responses with full text and delta.

        Raises:
            CallServerLLMError: If the streaming call fails.
        """
        config = self._make_generate_config(**kwargs)
        try:
            stream = self._client.models.generate_content_stream(
                model=self.config.model,
                contents=prompt,
                config=config,
            )

            full_text = ""
            for chunk in stream:
                chunk_text = self._extract_text(chunk)
                # Only process chunks that contain actual text content
                if delta := chunk_text:
                    full_text += delta
                    yield CompletionResponse(text=full_text, delta=delta)
        except APIError as e:
            raise CallServerLLMError(
                f"Google Gen-AI stream call failed: {e.message}"
            ) from e

    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """
        Generates a single, non-streaming completion response asynchronously.

        Args:
            prompt (str): The user input prompt to process.
            **kwargs: Additional parameters to override config defaults.

        Returns:
            CompletionResponse: The model's response with generated text.

        Raises:
            CallServerLLMError: If the async API call fails.
        """
        config = self._make_generate_config(**kwargs)
        try:
            response = await self._client.aio.models.generate_content(
                model=self.config.model,
                contents=prompt,
                config=config,
            )

            text = self._extract_text(response)
            if not text:
                raise CallServerLLMError(
                    "Async Google Gen-AI API returned empty response text."
                )

            return CompletionResponse(text=text)
        except APIError as e:
            raise CallServerLLMError(
                f"Async Google Gen-AI API call failed: {e.message}"
            ) from e

    async def astream_complete(  # type: ignore[override]
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseAsyncGen:
        """
        Generates a streaming completion response asynchronously.

        This method provides async streaming capabilities, yielding incremental
        responses as they become available in an async context.

        Args:
            prompt (str): The user input prompt to process.
            **kwargs: Additional parameters to override config defaults.

        Yields:
            CompletionResponse: Incremental responses with full text and delta.

        Raises:
            CallServerLLMError: If the async streaming call fails.
        """
        config = self._make_generate_config(**kwargs)
        try:
            stream = await self._client.aio.models.generate_content_stream(
                model=self.config.model,
                contents=prompt,
                config=config,
            )

            full_text = ""
            async for chunk in stream:
                chunk_text = self._extract_text(chunk)
                # Only process chunks that contain actual text content
                if delta := chunk_text:
                    full_text += delta
                    yield CompletionResponse(text=full_text, delta=delta)
        except APIError as e:
            raise CallServerLLMError(
                f"Async Google Gen-AI stream call failed: {e.message}"
            ) from e
