{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Comparison: Deep Agent vs RAG Basic\n",
    "\n",
    "This notebook compares two approaches for answering marketing questions:\n",
    "1. **Deep Agent**: Uses `search_knowledge_graph` + `search_document_library` tools with agentic reasoning\n",
    "2. **RAG Basic**: Direct hybrid search + LLM response (no tools)\n",
    "\n",
    "## Evaluation\n",
    "- Test Questions: Marketing domain with ground truth answers\n",
    "- Evaluator: Gemini-2.5-Pro with structured judgment\n",
    "- Metrics: Accuracy (correct vs incorrect), reasoning quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    ClearToolUsesEdit,\n",
    "    ContextEditingMiddleware,\n",
    "    SummarizationMiddleware,\n",
    "    ToolRetryMiddleware,\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from deepagents.middleware.patch_tool_calls import PatchToolCallsMiddleware\n",
    "from config.system_config import SETTINGS\n",
    "from shared.agent_middlewares import (\n",
    "    EnsureTasksFinishedMiddleware,\n",
    "    LogModelMessageMiddleware,\n",
    ")\n",
    "from shared.agent_tools import TodoWriteMiddleware\n",
    "from shared.agent_tools.retrieval import search_document_library, search_knowledge_graph\n",
    "from shared.model_clients.llm.google import GoogleAIClientLLM, GoogleAIClientLLMConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Questions & Ground Truth\n",
    "\n",
    "Marketing questions with verified answers for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 test questions\n"
     ]
    }
   ],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    {\n",
    "        \"question\": \"What is market segmentation and why is it important?\",\n",
    "        \"ground_truth\": \"Market segmentation is the process of dividing a broad target market into subsets of consumers with common needs or characteristics. It is important because it allows companies to target specific customer groups more effectively, allocate marketing resources efficiently, and create tailored value propositions that resonate with different segments.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the 4Ps of marketing?\",\n",
    "        \"ground_truth\": \"The 4Ps of marketing are Product (what you sell), Price (how much you charge), Place (where you sell), and Promotion (how you communicate). Together they form the marketing mix that companies use to pursue their marketing objectives.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does pricing strategy affect customer perceived value?\",\n",
    "        \"ground_truth\": \"Pricing strategy directly influences customer perceived value by signaling quality, positioning the brand, and affecting the cost-benefit analysis customers make. Premium pricing can enhance perceived quality and exclusivity, while value pricing emphasizes affordability. The price must align with the value proposition to maintain customer satisfaction and competitive positioning.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between push and pull marketing strategies?\",\n",
    "        \"ground_truth\": \"Push marketing involves proactively pushing products to customers through direct sales, trade promotions, and distribution channels. Pull marketing focuses on creating demand that pulls customers to the product through advertising, brand building, and consumer promotions. Push is manufacturer-driven, while pull is consumer-driven.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is brand positioning and why does it matter?\",\n",
    "        \"ground_truth\": \"Brand positioning is the process of establishing a distinct place in the minds of target customers relative to competitors. It matters because it differentiates the brand, creates a unique value proposition, influences purchase decisions, and guides all marketing communications and strategies. Strong positioning leads to brand preference and loyalty.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(TEST_QUESTIONS)} test questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Agent Setup\n",
    "\n",
    "Create Deep Agent with all middlewares from Cartographer (excluding filesystem/line_search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deep Agent configuration ready\n"
     ]
    }
   ],
   "source": [
    "DEEP_AGENT_INSTRUCTION = \"\"\"# ROLE & OBJECTIVE\n",
    "You are **The Deep Marketing Analyst**, a specialized AI consultant for marketing strategy and theory.\n",
    "Your mission is to answer user inquiries by synthesizing verified information from two internal databases: a **Knowledge Graph** (Cognitive Map) and a **Document Library** (Evidence Archive).\n",
    "\n",
    "**CORE PHILOSOPHY: RESEARCH-FIRST IMPLEMENTATION**\n",
    "* **\"Understand first, do later\":** Never attempt to answer or search for details until you have constructed a solid mental model of the domain.\n",
    "* **No Blind Searching:** Do not blindly search the Document Library with vague keywords. You must know *what* you are looking for and *where* it is likely located (Book, Chapter, Context) before you dig.\n",
    "* **Evidence-Based:** Your knowledge comes from the provided tools, not your pre-training. Always ground your answers in the retrieved context.\n",
    "\n",
    "# YOUR TOOLBOX\n",
    "\n",
    "1.  `search_knowledge_graph(query)`: **The Strategist (Brain)**\n",
    "    * *Use this FIRST.*\n",
    "    * Returns: Concepts, relationships, principles, causal logic (\"Why/How\"), and **Source Pointers** (Book names, Chapters).\n",
    "    * *Goal:* To brainstorm, understand the problem space, and identify *where* to look for details.\n",
    "\n",
    "2.  `search_document_library(query, filters...)`: **The Archivist (Memory)**\n",
    "    * *Use this SECOND (Conditional).*\n",
    "    * Returns: Raw text passages, exact quotes, data tables, and specific examples.\n",
    "    * *Goal:* To verify facts, get specific execution details, or retrieve exact citations based on the \"leads\" found in the Knowledge Graph.\n",
    "\n",
    "# COGNITIVE WORKFLOW\n",
    "\n",
    "## Phase 1: Sensemaking & Mapping (Mandatory)\n",
    "* **Action:** Always start by consulting `search_knowledge_graph` with a conceptual query.\n",
    "* **Reasoning:**\n",
    "    * What concepts are involved? How do they relate?\n",
    "    * What is the underlying logic or strategy?\n",
    "    * *Crucial:* Note the **Source Metadata** returned by the KG (e.g., \"This concept is discussed in Chapter 5 of Kotler's book\").\n",
    "\n",
    "## Phase 2: Sufficiency Assessment (The Decision Point)\n",
    "Review the information retrieved from the Knowledge Graph. Ask yourself:\n",
    "* *\"Is this conceptual understanding sufficient to answer the user fully?\"*\n",
    "    * **YES (Conceptual/Strategic Questions):** If the user asks \"What is the principle of 4P?\", the KG's explanation is likely enough. -> **Proceed to Answer.**\n",
    "    * **NO (Detailed/Executional Questions):** If the user asks for \"Examples of IKEA's pricing\" or \"The exact steps to launch,\" the KG might be too abstract. -> **Proceed to Phase 3.**\n",
    "\n",
    "## Phase 3: Targeted Deep Dive (Conditional)\n",
    "* **Action:** Use `search_document_library` to fetch missing details.\n",
    "* **Precision Targeting:** Do NOT search globally. Use the **metadata** found in Phase 1 to narrow your search.\n",
    "    * *Bad:* Search \"pricing\" across the whole library.\n",
    "    * *Good:* Search \"product mix pricing examples\" with `filter_by_chapter=\"Chapter 10\"`.\n",
    "\n",
    "## Phase 4: Synthesis & Response\n",
    "Construct your final answer by weaving together the **Logic** (from KG) and the **Evidence** (from Library).\n",
    "* **Structure:** Define the concept -> Explain the mechanics -> Provide specific examples/evidence.\n",
    "* **Citation:** Explicitly mention where the information comes from (e.g., \"According to Chapter 7...\").\n",
    "\n",
    "# GUIDELINES FOR SUCCESS\n",
    "1.  **Don't Over-Tool:** If the Knowledge Graph gives you a perfect, comprehensive answer, do not waste time calling the Document Library just for the sake of it.\n",
    "2.  **Contextual Querying:** When calling tools, write queries that match the tool's nature.\n",
    "    * *KG Query:* Abstract & Relational (e.g., \"Drivers of Customer Loyalty\").\n",
    "    * *Doc Query:* Specific & Lexical (e.g., \"Customer Loyalty program case studies\").\n",
    "3.  **Be Transparent:** If you cannot find information in either tool, admit it. Do not hallucinate marketing theories.\n",
    "\"\"\"\n",
    "\n",
    "def create_deep_agent():\n",
    "    \"\"\"Create Deep Agent with all middlewares from Cartographer (minus filesystem/line_search).\"\"\"\n",
    "    # 1. Initialize Gemini model (same config as Cartographer)\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        google_api_key=SETTINGS.GEMINI_API_KEY,\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        temperature=0.1,\n",
    "        thinking_budget=2000,\n",
    "        max_output_tokens=4000,\n",
    "        include_thoughts=False,\n",
    "    )\n",
    "    model_context_window = 1048576  # 1M tokens\n",
    "    \n",
    "    # 2. Setup Middlewares (same as Cartographer, excluding filesystem)\n",
    "    todo_middleware = TodoWriteMiddleware()\n",
    "    patch_middleware = PatchToolCallsMiddleware()\n",
    "    retry_middleware = ToolRetryMiddleware()\n",
    "    stop_check_middleware = EnsureTasksFinishedMiddleware()\n",
    "    log_message_middleware = LogModelMessageMiddleware(\n",
    "        log_thinking=True,\n",
    "        log_text_response=False,\n",
    "        log_tool_calls=True,\n",
    "        log_tool_results=True,\n",
    "        truncate_thinking=1000,\n",
    "        truncate_tool_results=1000,\n",
    "        exclude_tools=[],  # Log all tools\n",
    "    )\n",
    "    context_edit_middleware = ContextEditingMiddleware(\n",
    "        edits=[\n",
    "            ClearToolUsesEdit(\n",
    "                trigger=100000,  # Clear after 100k tokens\n",
    "                keep=5,  # Keep last 5 tool results\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    msg_summary_middleware = SummarizationMiddleware(\n",
    "        model=model,\n",
    "        trigger=(\n",
    "            \"tokens\",\n",
    "            int(model_context_window * 0.6),  # Summarize at 60% context\n",
    "        ),\n",
    "        keep=(\"messages\", 20),  # Keep last 20 messages\n",
    "    )\n",
    "    \n",
    "    # 3. Create agent with KG and Doc tools + all middlewares\n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=[search_knowledge_graph, search_document_library],\n",
    "        system_prompt=DEEP_AGENT_INSTRUCTION,\n",
    "        middleware=[\n",
    "            context_edit_middleware,\n",
    "            msg_summary_middleware,\n",
    "            todo_middleware,\n",
    "            patch_middleware,\n",
    "            log_message_middleware,\n",
    "            retry_middleware,\n",
    "            stop_check_middleware,\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return agent, model\n",
    "\n",
    "print(\"✓ Deep Agent configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Basic Setup\n",
    "\n",
    "Simple hybrid search + LLM response (no tools, no agentic reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG Basic configuration ready\n"
     ]
    }
   ],
   "source": [
    "RAG_BASIC_INSTRUCTION = \"\"\"# ROLE & OBJECTIVE\n",
    "You are **The Marketing Knowledge Assistant**, an AI that answers marketing questions based on provided context.\n",
    "\n",
    "**YOUR TASK:**\n",
    "Answer the user's question using ONLY the information provided in the \"Retrieved Context\" section below.\n",
    "\n",
    "**GUIDELINES:**\n",
    "1. **Stay Grounded:** Base your answer strictly on the retrieved context. Do not use your pre-training knowledge.\n",
    "2. **Be Comprehensive:** If the context contains relevant information, provide a complete answer covering all key points.\n",
    "3. **Be Honest:** If the retrieved context does not contain enough information to answer the question, say \"I don't have enough information in the retrieved context to answer this question fully.\"\n",
    "4. **Structure Well:** Organize your answer clearly with definitions, explanations, and examples when available.\n",
    "5. **Cite Sources:** Mention where information comes from (e.g., \"According to the retrieved passage...\").\n",
    "\n",
    "**OUTPUT FORMAT:**\n",
    "Provide a direct, well-structured answer. Do not add meta-commentary about the retrieval process.\n",
    "\"\"\"\n",
    "\n",
    "async def rag_basic_answer(question: str) -> str:\n",
    "    \"\"\"RAG Basic: Direct search + LLM response.\"\"\"\n",
    "    # Step 1: Retrieve context via hybrid search\n",
    "    retrieved_context = await search_document_library(\n",
    "        query=question,\n",
    "        max_results=10,\n",
    "    )\n",
    "    \n",
    "    # Step 2: Build prompt with retrieved context\n",
    "    prompt = f\"\"\"# RETRIEVED CONTEXT\n",
    "\n",
    "{retrieved_context}\n",
    "\n",
    "---\n",
    "\n",
    "# USER QUESTION\n",
    "\n",
    "{question}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the retrieved context above.\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 3: Get LLM response\n",
    "    llm = GoogleAIClientLLM(\n",
    "        config=GoogleAIClientLLMConfig(\n",
    "            model=\"gemini-2.5-flash-lite\",\n",
    "            api_key=SETTINGS.GEMINI_API_KEY,\n",
    "            system_instruction=RAG_BASIC_INSTRUCTION,\n",
    "            temperature=0.1,\n",
    "            thinking_budget=2000,\n",
    "            max_tokens=4000,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response = await llm.acomplete(prompt)\n",
    "    return response.text\n",
    "\n",
    "print(\"✓ RAG Basic configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluator Setup\n",
    "\n",
    "Gemini-2.5-Pro evaluates responses against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluator configuration ready\n"
     ]
    }
   ],
   "source": [
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Structured evaluation result.\"\"\"\n",
    "    is_correct: bool = Field(..., description=\"Whether the answer is correct\")\n",
    "    reasoning: str = Field(..., description=\"Explanation of the judgment\")\n",
    "\n",
    "EVALUATOR_TASK_PROMPT = \"\"\"# ROLE & OBJECTIVE\n",
    "You are **The Truth Arbiter**, an expert evaluator responsible for verifying the accuracy of AI-generated responses.\n",
    "Your specific task is to compare an **AGENT_RESPONSE** against a **GROUND_TRUTH** (the correct answer) for a given **QUESTION**.\n",
    "\n",
    "**YOUR CORE JUDGMENT PHILOSOPHY:**\n",
    "You are judging based on **Information Containment**, not Keyword Matching.\n",
    "* **CORRECT:** The Agent's response conveys the *same core meaning* and facts as the Ground Truth, even if the wording, length, or structure is different. Extra context provided by the Agent is acceptable as long as it does not contradict the truth.\n",
    "* **INCORRECT:** The Agent's response contradicts the Ground Truth, fails to answer the core question, or omits the most critical piece of information required by the Ground Truth.\n",
    "\n",
    "# INPUT DATA\n",
    "\n",
    "**1. QUESTION:**\n",
    "```\n",
    "{{QUESTION}}\n",
    "```\n",
    "\n",
    "**2. GROUND_TRUTH (The Standard):**\n",
    "```\n",
    "{{GROUND_TRUTH}}\n",
    "```\n",
    "\n",
    "**3. AGENT_RESPONSE (The Candidate):**\n",
    "```\n",
    "{{AGENT_RESPONSE}}\n",
    "```\n",
    "\n",
    "# EVALUATION WORKFLOW\n",
    "\n",
    "1.  **Extract Key Facts:** Identify the essential facts, numbers, or concepts in the `GROUND_TRUTH` that *must* be present for the answer to be valid.\n",
    "2.  **Verify Presence:** Check if these essential facts are present in the `AGENT_RESPONSE`.\n",
    "    * *Allow:* Paraphrasing, synonyms, and summarization.\n",
    "    * *Allow:* Additional relevant details (unless they are explicitly wrong).\n",
    "3.  **Determine Verdict:**\n",
    "    * If the core facts are present and accurate -> **true**.\n",
    "    * If core facts are missing, wrong, or hallucinated -> **false**.\n",
    "\"\"\"\n",
    "\n",
    "async def evaluate_response(\n",
    "    question: str,\n",
    "    ground_truth: str,\n",
    "    agent_response: str,\n",
    ") -> EvaluationResult:\n",
    "    \"\"\"Evaluate agent response against ground truth.\"\"\"\n",
    "    # Build evaluation prompt\n",
    "    prompt = (\n",
    "        EVALUATOR_TASK_PROMPT\n",
    "        .replace(\"{{QUESTION}}\", question)\n",
    "        .replace(\"{{GROUND_TRUTH}}\", ground_truth)\n",
    "        .replace(\"{{AGENT_RESPONSE}}\", agent_response)\n",
    "    )\n",
    "    \n",
    "    # Use Gemini-2.5-Pro for evaluation\n",
    "    evaluator = GoogleAIClientLLM(\n",
    "        config=GoogleAIClientLLMConfig(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            api_key=SETTINGS.GEMINI_API_KEY,\n",
    "            temperature=0.0,  # Deterministic evaluation\n",
    "            max_tokens=1000,\n",
    "            thinking_budget=2000,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=EvaluationResult,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response = await evaluator.acomplete(prompt)\n",
    "    result = json.loads(response.text)\n",
    "    return EvaluationResult(**result)\n",
    "\n",
    "print(\"✓ Evaluator configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Comparison\n",
    "\n",
    "Test both approaches on all questions and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Deep Agent...\n",
      "\n",
      "================================================================================\n",
      "Question 1/5: What is market segmentation and why is it important?\n",
      "================================================================================\n",
      "\n",
      "[Deep Agent] Processing...\n",
      "Error: Expected dict, got What is market segmentation and why is it important?\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "\n",
      "[RAG Basic] Processing...\n",
      "Error: search_document_library() got an unexpected keyword argument 'max_results'\n",
      "\n",
      "================================================================================\n",
      "Question 2/5: What are the 4Ps of marketing?\n",
      "================================================================================\n",
      "\n",
      "[Deep Agent] Processing...\n",
      "Error: Expected dict, got What are the 4Ps of marketing?\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "\n",
      "[RAG Basic] Processing...\n",
      "Error: search_document_library() got an unexpected keyword argument 'max_results'\n",
      "\n",
      "================================================================================\n",
      "Question 3/5: How does pricing strategy affect customer perceived value?\n",
      "================================================================================\n",
      "\n",
      "[Deep Agent] Processing...\n",
      "Error: Expected dict, got How does pricing strategy affect customer perceived value?\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "\n",
      "[RAG Basic] Processing...\n",
      "Error: search_document_library() got an unexpected keyword argument 'max_results'\n",
      "\n",
      "================================================================================\n",
      "Question 4/5: What is the difference between push and pull marketing strategies?\n",
      "================================================================================\n",
      "\n",
      "[Deep Agent] Processing...\n",
      "Error: Expected dict, got What is the difference between push and pull marketing strategies?\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "\n",
      "[RAG Basic] Processing...\n",
      "Error: search_document_library() got an unexpected keyword argument 'max_results'\n",
      "\n",
      "================================================================================\n",
      "Question 5/5: What is brand positioning and why does it matter?\n",
      "================================================================================\n",
      "\n",
      "[Deep Agent] Processing...\n",
      "Error: Expected dict, got What is brand positioning and why does it matter?\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "\n",
      "[RAG Basic] Processing...\n",
      "Error: search_document_library() got an unexpected keyword argument 'max_results'\n"
     ]
    }
   ],
   "source": [
    "async def run_comparison():\n",
    "    \"\"\"Run full comparison between Deep Agent and RAG Basic.\"\"\"\n",
    "    results = {\n",
    "        \"deep_agent\": [],\n",
    "        \"rag_basic\": [],\n",
    "    }\n",
    "    \n",
    "    # Initialize Deep Agent\n",
    "    print(\"Initializing Deep Agent...\")\n",
    "    deep_agent, _ = create_deep_agent()\n",
    "    \n",
    "    for i, test_case in enumerate(TEST_QUESTIONS, 1):\n",
    "        question = test_case[\"question\"]\n",
    "        ground_truth = test_case[\"ground_truth\"]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Question {i}/{len(TEST_QUESTIONS)}: {question}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Test Deep Agent\n",
    "        print(\"\\n[Deep Agent] Processing...\")\n",
    "        try:\n",
    "            deep_response = await deep_agent.ainvoke(question)\n",
    "            deep_answer = deep_response.get(\"output\", \"No response\")\n",
    "            print(f\"Answer: {deep_answer[:200]}...\")\n",
    "            \n",
    "            # Evaluate\n",
    "            deep_eval = await evaluate_response(question, ground_truth, deep_answer)\n",
    "            print(f\"Evaluation: {'✓ CORRECT' if deep_eval.is_correct else '✗ INCORRECT'}\")\n",
    "            print(f\"Reasoning: {deep_eval.reasoning}\")\n",
    "            \n",
    "            results[\"deep_agent\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": deep_answer,\n",
    "                \"is_correct\": deep_eval.is_correct,\n",
    "                \"reasoning\": deep_eval.reasoning,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results[\"deep_agent\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"ERROR: {e}\",\n",
    "                \"is_correct\": False,\n",
    "                \"reasoning\": \"Failed to generate response\",\n",
    "            })\n",
    "        \n",
    "        # Test RAG Basic\n",
    "        print(\"\\n[RAG Basic] Processing...\")\n",
    "        try:\n",
    "            rag_answer = await rag_basic_answer(question)\n",
    "            print(f\"Answer: {rag_answer[:200]}...\")\n",
    "            \n",
    "            # Evaluate\n",
    "            rag_eval = await evaluate_response(question, ground_truth, rag_answer)\n",
    "            print(f\"Evaluation: {'✓ CORRECT' if rag_eval.is_correct else '✗ INCORRECT'}\")\n",
    "            print(f\"Reasoning: {rag_eval.reasoning}\")\n",
    "            \n",
    "            results[\"rag_basic\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": rag_answer,\n",
    "                \"is_correct\": rag_eval.is_correct,\n",
    "                \"reasoning\": rag_eval.reasoning,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results[\"rag_basic\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"ERROR: {e}\",\n",
    "                \"is_correct\": False,\n",
    "                \"reasoning\": \"Failed to generate response\",\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "results = await run_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "Calculate accuracy and display comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze and display comparison results.\"\"\"\n",
    "    deep_correct = sum(1 for r in results[\"deep_agent\"] if r[\"is_correct\"])\n",
    "    rag_correct = sum(1 for r in results[\"rag_basic\"] if r[\"is_correct\"])\n",
    "    total = len(TEST_QUESTIONS)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nDeep Agent Accuracy: {deep_correct}/{total} ({deep_correct/total*100:.1f}%)\")\n",
    "    print(f\"RAG Basic Accuracy:  {rag_correct}/{total} ({rag_correct/total*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED BREAKDOWN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, (deep, rag) in enumerate(zip(results[\"deep_agent\"], results[\"rag_basic\"]), 1):\n",
    "        print(f\"\\nQ{i}: {deep['question']}\")\n",
    "        print(f\"  Deep Agent: {'✓' if deep['is_correct'] else '✗'} | {deep['reasoning']}\")\n",
    "        print(f\"  RAG Basic:  {'✓' if rag['is_correct'] else '✗'} | {rag['reasoning']}\")\n",
    "    \n",
    "    # Save results to JSON\n",
    "    output_path = Path(\"baseline_comparison_results.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\n✓ Results saved to {output_path}\")\n",
    "\n",
    "analyze_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
