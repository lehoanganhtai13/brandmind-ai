# Task 29: Refactor Search Web Module with Provider Architecture

## ğŸ“Œ Metadata

- **Epic**: Search Infrastructure
- **Priority**: High
- **Estimated Effort**: 1 week
- **Team**: Backend
- **Related Tasks**: N/A
- **Blocking**: []
- **Blocked by**: @suneox

### âœ… Progress Checklist

- [x] ğŸ¯ [Context & Goals](#ğŸ¯-context--goals) - Problem definition and success metrics
- [x] ğŸ›  [Solution Design](#ğŸ› -solution-design) - Architecture and technical approach
- [x] ğŸ”„ [Implementation Plan](#ğŸ”„-implementation-plan) - Detailed execution phases
- [x] ğŸ“‹ [Implementation Detail](#ğŸ“‹-implementation-detail) - Component requirements
    - [x] âœ… [Component 1: Data Models](#component-1-data-models) - ProviderResult
    - [x] âœ… [Component 2: Base Provider](#component-2-base-provider) - Abstract base class
    - [x] âœ… [Component 3: Providers](#component-3-providers) - All 5 providers
    - [x] âœ… [Component 4: Main Orchestrator](#component-4-main-orchestrator) - search_web function
- [x] âœ… [Test Cases](#ğŸ§ª-test-cases) - Manual test cases and validation
- [x] âœ… [Task Summary](#ğŸ“-task-summary) - Final implementation summary

## ğŸ”— Reference Documentation

- **Coding Standards**: Follow enterprise-level Python standards
- **Model Organization Pattern**:
  - **Shared Models** (`utils/base_class.py`): Models used across multiple modules (e.g., `SearchResult`, `ScrapeResult`)
  - **Internal Models** (module's `models.py`): Models only used within that module (e.g., `ProviderResult` - only used by search providers internally)

------------------------------------------------------------------------

## ğŸ¯ Context & Goals

### Bá»‘i cáº£nh

- `search_web.py` chá»©a 5 search functions trong 1 file (732 lines)
- Xá»­ lÃ½ tuáº§n tá»± tá»«ng query, khÃ´ng táº­n dá»¥ng concurrency
- Hardcode SearXNG engines, khÃ´ng flexible cho viá»‡c thÃªm providers
- CÃ¡c functions hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p, chÆ°a Ä‘Æ°á»£c integrate vÃ o chain

### Má»¥c tiÃªu

Enhance `search_web` vá»›i:
1. **Parallel Processing**: Max 3 queries concurrent per batch
2. **Engine Rotation**: Try engines within provider, failed â†’ next engine
3. **Provider Chain**: SearXNG â†’ Bing â†’ Tavily â†’ Perplexity â†’ Scrapeless
4. **Flexible Architecture**: Dá»… thÃªm/bá» providers

### Success Metrics / Acceptance Criteria

- **Performance**: 3x faster vá»›i 3 concurrent queries
- **Maintainability**: Má»—i provider < 200 lines
- **Extensibility**: ThÃªm provider má»›i = 1 file + 1 config line
- **Reliability**: Failed queries retry trÃªn next engine/provider

------------------------------------------------------------------------

## ğŸ›  Solution Design

### Giáº£i phÃ¡p Ä‘á» xuáº¥t

**Provider Chain Architecture**: TÃ¡ch táº¥t cáº£ 5 search functions thÃ nh providers riÃªng, orchestrate bá»Ÿi `search_web()` vá»›i batch processing vÃ  engine rotation.

### Existing Components (Reuse)

- **`SearchResult`**: Pydantic BaseModel táº¡i `shared/utils/base_class.py`

### New Components

- **`ProviderResult`**: Pydantic model cho return type cá»§a providers
- **`BaseProvider`**: Abstract base class vá»›i `search()` method
- **5 Provider Classes**: SearXNGProvider, BingProvider, TavilyProvider, PerplexityProvider, ScrapelessProvider

### Provider Chain Order

```
PROVIDERS = [
    SearXNGProvider,      # engines: duckduckgo, startpage
    BingProvider,         # direct curl
    TavilyProvider,       # API
    PerplexityProvider,   # API  
    ScrapelessProvider,   # API (deep_serp_search)
]
```

------------------------------------------------------------------------

## ğŸ”„ Implementation Plan

### **Phase 1: Foundation**
1. **Data Models**
   - Create `ProviderResult` Pydantic model
   - Reuse existing `SearchResult`

2. **Base Provider**
   - Create `providers/base.py` vá»›i `BaseProvider` ABC

### **Phase 2: Extract All Providers**
1. **SearXNGProvider** - Extract tá»« `search_web()` inner logic
2. **BingProvider** - Extract tá»« `bing_web_search()`
3. **TavilyProvider** - Extract tá»« `tavily_search()`
4. **PerplexityProvider** - Extract tá»« `perplexity_search()`
5. **ScrapelessProvider** - Extract tá»« `deep_serp_search()`

### **Phase 3: Orchestrator**
1. **Refactor search_web.py**
   - Provider chain config
   - Batch processing (max 3 concurrent)
   - Engine rotation logic
   - Global availability tracking

------------------------------------------------------------------------

## ğŸ“‹ Implementation Detail

> **ğŸ“ Coding Standards & Documentation Requirements**
>
> All code implementations **MUST** follow **enterprise-level Python standards**:
>
> - **Comprehensive Docstrings**: Every module, class, and function must have detailed docstrings in English explaining:
>   - **Purpose**: What this component does and why it exists
>   - **Functionality**: How it processes data and what transformations occur
>   - **Data Types**: Input/output types and data structures
>   - **Business Logic**: How it fits into the overall workflow
>
> - **Detailed Comments**: Add inline comments explaining complex logic, business rules, and decision points
>
> - **Consistent String Quoting**: Use double quotes `"` consistently throughout all code (not single quotes `'`)
>
> - **Focus on Functionality**: Document the "what" and "why" rather than the "how" - explain business purpose, not code implementation details
>
> - **Language**: All code, comments, and docstrings must be in **English only**
>
> - **Naming Conventions**: Follow PEP 8 naming conventions for variables, functions, classes, and modules
>
> - **Modularization**: Break down large functions/classes into smaller, reusable components with clear responsibilities
>
> - **Type Hints**: Use Python type hints for all function signatures to ensure clarity on expected data types
>
> - **Line Length**: Max 100 characters - break long lines for readability

### Component 1: Data Models

#### Requirement 1 - ProviderResult Model
- **Requirement**: Strongly-typed return object cho providers
- **Implementation**:
  - `search/models.py`
  ```python
  from typing import Dict, List
  from pydantic import BaseModel, Field
  from shared.utils.base_class import SearchResult
  
  class ProviderResult(BaseModel):
      """
      Encapsulates the result of a provider search operation.
      
      Attributes:
          success_results: Dict mapping query -> list of SearchResult
          failed_queries: List of queries that failed
          engine_used: Name of engine/provider used
          response_time: Time taken for search operation
      """
      success_results: Dict[str, List[SearchResult]] = Field(default_factory=dict)
      failed_queries: List[str] = Field(default_factory=list)
      engine_used: str = ""
      response_time: float = 0.0
  ```
- **Note**: Sá»­ dá»¥ng Pydantic `BaseModel` Ä‘á»ƒ consistent vá»›i `SearchResult`

### Component 2: Base Provider

#### Requirement 1 - Abstract Base Class
- **Implementation**:
  - `search/providers/base.py`
  ```python
  from abc import ABC, abstractmethod
  from typing import List, Optional
  from ..models import ProviderResult
  
  class BaseProvider(ABC):
      """Abstract base class for all search providers."""
      
      name: str
      engines: List[Optional[str]]  # None for single-engine providers
      
      @abstractmethod
      def search(
          self,
          queries: List[str],
          engine: Optional[str],
          num_results: int
      ) -> ProviderResult:
          """Execute search for given queries."""
          pass
  ```

### Component 3: Providers

#### Provider 1 - SearXNGProvider
- **File**: `search/providers/searxng.py`
- **Source**: Extract tá»« `search_web()` inner logic (lines 540-689)
- **Engines**: `["duckduckgo", "startpage"]`
- **Logic**: Call SearXNG API vá»›i specified engine

#### Provider 2 - BingProvider
- **File**: `search/providers/bing.py`
- **Source**: Extract tá»« `bing_web_search()` (lines 270-473)
- **Engines**: `[None]` (single engine)
- **Logic**: Curl-based HTML scraping

#### Provider 3 - TavilyProvider
- **File**: `search/providers/tavily.py`
- **Source**: Extract tá»« `tavily_search()` (lines 184-267)
- **Engines**: `[None]`
- **Logic**: Tavily API vá»›i TAVILY_API_KEY

#### Provider 4 - PerplexityProvider
- **File**: `search/providers/perplexity.py`
- **Source**: Extract tá»« `perplexity_search()` (lines 102-181)
- **Engines**: `[None]`
- **Logic**: Perplexity API vá»›i PERPLEXITY_API_KEY

#### Provider 5 - ScrapelessProvider
- **File**: `search/providers/scrapeless.py`
- **Source**: Extract tá»« `deep_serp_search()` (lines 34-99)
- **Engines**: `[None]`
- **Logic**: Scrapeless API vá»›i SCRAPELESS_API_KEY

### Component 4: Main Orchestrator

#### Requirement 1 - Enhanced search_web Function
- **File**: `search/search_web.py`
- **Implementation**:
  ```python
  MAX_QUERIES = 5
  MAX_BATCH_SIZE = 3
  BASE_DELAY = 3.5
  
  PROVIDERS: List[BaseProvider] = [
      SearXNGProvider(),
      BingProvider(),
      TavilyProvider(),
      PerplexityProvider(),
      ScrapelessProvider(),
  ]
  
  # Global: Track engine availability across calls
  _engine_available_at: Dict[str, float] = {}
  
  def search_web(queries: List[str], number_of_results: int = 10) -> Dict[str, Any]:
      """
      Main search orchestrator with provider chain and batch processing.
      
      Flow:
      1. Validate & dedupe queries (max 5)
      2. Split into batches (max 3 per batch)
      3. For each batch:
         - Find available engine (check _engine_available_at)
         - Process batch concurrently with ThreadPoolExecutor
         - Mark engine busy: delay = 3.5s Ã— batch_size
         - Collect results, track failed queries
      4. Failed queries â†’ retry with next engine/provider
      5. Return aggregated results
      """
  ```

#### Algorithm
```python
from shared.agent_tools.search.exceptions import SearchValidationError

# Input validation
queries = list(dict.fromkeys(queries))  # Dedupe
if len(queries) > MAX_QUERIES:
    raise SearchValidationError(
        message=f"Too many queries. Maximum allowed is {MAX_QUERIES}, got {len(queries)}",
        field="queries",
        value=len(queries)
    )
if not queries:
    raise SearchValidationError(
        message="Queries list cannot be empty",
        field="queries",
        value=queries
    )

remaining_queries = queries

for provider in PROVIDERS:
    for engine in provider.engines:
        if not remaining_queries:
            break
        
        # Check availability
        engine_key = f"{provider.name}_{engine}"
        wait = _engine_available_at.get(engine_key, 0) - now
        if wait > 0:
            sleep(wait)
        
        # Process batch (max 3 concurrent)
        batch = remaining_queries[:MAX_BATCH_SIZE]
        result = provider.search(batch, engine, num_results)
        
        # Update state
        _engine_available_at[engine_key] = now + (BASE_DELAY * len(batch))
        all_results.update(result.success_results)
        remaining_queries = result.failed_queries + remaining_queries[MAX_BATCH_SIZE:]
```

------------------------------------------------------------------------

## ğŸ§ª Test Cases

### Test Case 1: Parallel Batch Processing
- **Purpose**: Verify 3 queries processed concurrently
- **Steps**:
  1. Call `search_web(["q1", "q2", "q3"])`
  2. Monitor timing
- **Expected Result**: All 3 processed in ~1 request time, not 3x
- **Status**: â³ Pending

### Test Case 2: Engine Rotation Within Provider
- **Purpose**: Failed queries retry on next engine
- **Steps**:
  1. Query fails on duckduckgo
  2. Check if retried on startpage
- **Expected Result**: Query processed by startpage
- **Status**: â³ Pending

### Test Case 3: Provider Chain Fallback
- **Purpose**: All SearXNG engines fail â†’ move to Bing
- **Steps**:
  1. Simulate SearXNG failure
  2. Check Bing fallback
- **Expected Result**: Query processed by Bing
- **Status**: â³ Pending

------------------------------------------------------------------------

## ğŸ“ Task Summary

> **âœ… Implementation Complete** - 2026-01-30

### Files Created/Modified

```
src/shared/src/shared/agent_tools/search/
â”œâ”€â”€ __init__.py              # [MODIFIED] Updated exports
â”œâ”€â”€ models.py                # [NEW] ProviderResult Pydantic model
â”œâ”€â”€ exceptions.py            # [MODIFIED] Added SearchValidationError
â”œâ”€â”€ search_web.py            # [REWRITTEN] Orchestrator with provider chain
â””â”€â”€ providers/
    â”œâ”€â”€ __init__.py          # [NEW] Provider exports
    â”œâ”€â”€ base.py              # [NEW] BaseProvider ABC + is_available() + requires_delay
    â”œâ”€â”€ searxng.py           # [NEW] Multi-engine provider + health check
    â”œâ”€â”€ bing.py              # [NEW] Direct curl scraping (fallback)
    â”œâ”€â”€ tavily.py            # [NEW] API provider + is_available()
    â”œâ”€â”€ perplexity.py        # [NEW] API provider + is_available()
    â””â”€â”€ scrapeless.py        # [NEW] API provider (disabled - no snippets)
```

### Key Achievements

1. **Provider Architecture**: Abstract `BaseProvider` class with 5 concrete implementations
2. **Strong Typing**: Pydantic `ProviderResult` model replaces tuple returns
3. **Input Validation**: `SearchValidationError` for explicit validation (max 5 queries)
4. **Batch Processing**: Concurrent execution with while-loop to exhaust working providers
5. **Production Standards**: Enterprise-level docstrings, type hints, 100-char line limit
6. **Provider Availability**: `is_available()` method to skip unavailable providers
   - API providers: Check environment variable for API key
   - SearXNG: HTTP health check with 2s timeout
7. **Rate Limiting Control**: `requires_delay` property (only SearXNG needs delay)
8. **Provider Order**: `SearXNG â†’ Perplexity â†’ Tavily â†’ Bing` (Bing last - inconsistent)
9. **Scrapeless Disabled**: Commented out due to missing snippets in results
10. **Smart Provider Exhaustion**: Keep using working provider until all queries done or failure

### Provider Chain (Active)

```
SearXNG (duckduckgo â†’ startpage) â†’ Perplexity â†’ Tavily â†’ Bing (fallback)
```

### Provider Properties

| Provider | `is_available()` | `requires_delay` |
|----------|------------------|------------------|
| SearXNG | HTTP health check | âœ… Yes |
| Perplexity | `PERPLEXITY_API_KEY` env | âŒ No |
| Tavily | `TAVILY_API_KEY` env | âŒ No |
| Scrapeless | `SCRAPELESS_API_KEY` env | âŒ No |
| Bing | Always True | âŒ No |

### Verification Results

- âœ… All imports successful
- âœ… All providers instantiate correctly
- âœ… ProviderResult model validates
- âœ… SearchValidationError works
- âœ… Input validation (empty/too many queries)
- âœ… `make typecheck` passes (mypy, ruff, bandit)
- âœ… Provider exhaustion logic works (while loop)
- âœ… SearXNG health check skips when service down

------------------------------------------------------------------------
