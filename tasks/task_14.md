# Task 14: Semantic Chunking Module - The Slicer (Stage 2)

## ðŸ“Œ Metadata

- **Epic**: Knowledge Graph RAG System
- **Priority**: High
- **Estimated Effort**: 2-3 days
- **Team**: Backend + AI/ML
- **Related Tasks**: Task 13 (Document Mapping)
- **Workflow Stage**: Stage 2 of 3 (Mapping â†’ **Chunking** â†’ Building)
- **Blocking**: Stage 3 (Knowledge Graph Building)
- **Blocked by**: Task 13 (requires `global_map.json`)

### âœ… Progress Checklist

- [x] ðŸŽ¯ [Context & Goals](#ðŸŽ¯-context--goals) - Problem definition and success metrics
- [x] ðŸ›  [Solution Design](#ðŸ› -solution-design) - Architecture and technical approach
- [x] ðŸ”„ [Implementation Plan](#ðŸ”„-implementation-plan) - Detailed execution phases
- [x] ðŸ“‹ [Implementation Detail](#ðŸ“‹-implementation-detail) - Component requirements
    - [x] ðŸ“ [Module Structure](#module-structure) - Directory setup
    - [x] ðŸ”§ [Core Components](#core-components) - Chunking logic
    - [x] ðŸ“Š [Models & Schemas](#models--schemas) - Data structures
    - [x] ðŸ”— [CLI Integration](#cli-integration) - Command-line interface
- [x] ðŸ§ª [Test Cases](#ðŸ§ª-test-cases) - Manual test cases and validation
- [x] ðŸ“ [Task Summary](#ðŸ“-task-summary) - Final implementation summary

## ðŸ”— Reference Documentation

- **Coding Standards**: Follow enterprise-level Python standards with comprehensive documentation
- **Related Files**:
  - `docs/brainstorm/discussion.md`: Full workflow specification
  - `docs/brainstorm/chunking_discuss.md`: Detailed chunking strategy discussion
  - `tasks/task_13.md`: Stage 1 (Document Mapping) implementation
  - `data/parsed_documents/.../global_map.json`: Input from Stage 1
- **Dependencies**: `langchain-text-splitters` (optional group: knowledge-graph)

------------------------------------------------------------------------

## ðŸŽ¯ Context & Goals

### Bá»‘i cáº£nh

We have successfully completed **Stage 1 (Document Mapping)** which generated `global_map.json` containing:
- Hierarchical structure (Parts â†’ Chapters â†’ Sections)
- Page ranges for each section
- **Exact start line numbers** for section headers
- Section summaries (hierarchical, from small to large)

**Current State**:
```
data/parsed_documents/Kotler_..._20251123_193123/
â”œâ”€â”€ page_1.md, page_2.md, ..., page_736.md   # Individual page files
â””â”€â”€ global_map.json                          # Structural map from Stage 1
```

**Problem**:
- Need to convert 736 individual page files into semantic chunks
- Chunks must respect section boundaries (no chunk can span across 2 sections)
- Each chunk needs rich metadata (source hierarchy, pages, document info)
- Chunks will feed into 2 downstream pipelines:
  - **Pipeline A**: Vector DB (document library)
  - **Pipeline B**: Knowledge extraction â†’ Graph DB

### Má»¥c tiÃªu

Build **The Slicer** module that:
1. Reads `global_map.json` to understand document structure
2. For each highest-level section (Chapter/Part):
   - Merges all pages of that section into one content string
   - **Removes metadata headers** from page files (lines 1-9)
   - Uses `start_line_index` to cut content precisely at section boundaries
3. Applies **Adaptive Structural Chunking** (paragraph-based, NOT semantic embedding):
   - Respects subsection boundaries (no chunk spans across subsections)
   - Aggregates paragraphs up to target size (~400 words)
   - Falls back to sentence splitting only for giant paragraphs (>700 words)
4. Implements **Character Offset Mapping** to track which pages each chunk belongs to
5. Outputs chunks with complete metadata ready for Stage 3

### Success Metrics / Acceptance Criteria

- **Correctness**:
  - 100% compliance: No chunk spans across 2 sections (any level)
  - Accurate page mapping: Each chunk knows which pages it came from
  - Metadata completeness: All required fields present
  
- **Performance**:
  - Process 736-page document in < 1 minute (pure code, no LLM calls)
  - Parallel processing: Batch of 8 chapters at a time
  - Memory efficient: Process batches sequentially
  
- **Quality**:
  - Chunk size distribution: 80% within 300-500 words
  - No information loss at section boundaries
  - Paragraph integrity maintained (no mid-paragraph cuts except for giant paragraphs)

------------------------------------------------------------------------

## ðŸ›  Solution Design

### Giáº£i phÃ¡p Ä‘á» xuáº¥t

**Adaptive Structural Chunking**: A hybrid approach that combines structural awareness (sections, paragraphs) with length control (token limits), using **zero overlap** to avoid duplicate triples in downstream knowledge extraction.

**Key Design Principles**:

1. **Structure-First**: Respect document structure (sections, paragraphs) over arbitrary token limits
2. **Zero Overlap**: No chunk overlap to prevent duplicate triple extraction
3. **Metadata-Rich**: Each chunk carries full context (hierarchy, pages, summary)
4. **Code-Based**: Pure Python logic (no LLM/embedding calls) for speed and cost

### Stack cÃ´ng nghá»‡

- **Text Splitting**: `langchain-text-splitters` package
  - Component: `RecursiveCharacterTextSplitter`
  - Purpose: Fallback for giant paragraphs (>700 words)
  - Why: Battle-tested, handles edge cases well
  
- **Word Counting**: Python built-in `str.split()`
  - Purpose: Simple and fast word counting
  - Why: No external dependencies, sufficient for chunking
  
- **Regex**: Python `re` module
  - Purpose: Parse page metadata headers, detect markdown structures
  - Why: Efficient for pattern matching

- **Data Models**: Pydantic
  - Purpose: Type-safe chunk models with validation
  - Why: Ensures data integrity, easy serialization

### Issues & Solutions

1. **Challenge**: Page files have metadata headers (lines 1-9) that must be removed when merging
   - **Solution**: Use regex to detect and strip metadata block:
     ```python
     # Pattern: Lines 1-9 contain metadata, line 10 is separator "---"
     # Remove everything up to and including the second "---"
     content = re.sub(r'^.*?---\n.*?---\n', '', page_content, count=1, flags=re.DOTALL)
     ```

2. **Challenge**: `start_line_index` in `global_map.json` includes metadata lines
   - **Solution**: Adjust line index by subtracting 10 (metadata header + separator)
     ```python
     actual_content_line = start_line_index - 10
     ```

3. **Challenge**: Tracking which pages a chunk belongs to after merging
   - **Solution**: Build **Character Offset Map** before chunking:
     ```python
     offset_map = {
         "page_5.md": (0, 1000),      # chars 0-1000
         "page_6.md": (1001, 2500),   # chars 1001-2500
     }
     # After chunking: chunk(start=1200, end=2300) â†’ pages=[5, 6]
     ```

4. **Challenge**: Paragraph aggregation without exceeding word limits
   - **Solution**: Greedy algorithm with lookahead:
     - Keep adding paragraphs while total < TARGET_SIZE (400 words)
     - If next paragraph would exceed, commit current chunk
     - Only split paragraph if single paragraph > MAX_SIZE (700 words)

5. **Challenge**: Handling subsection boundaries within a chapter
   - **Solution**: Recursive processing:
     - Process top-level section content first (before first subsection)
     - Then recursively process each subsection
     - Ensures chunks never span subsection boundaries

------------------------------------------------------------------------

## ðŸ”„ Implementation Plan

### **Phase 1: Module Setup & Models**
1. **Create Module Structure**
   - New module: `src/core/src/core/knowledge_graph/chunker/`
   - Models: `src/core/src/core/knowledge_graph/models/chunk.py`
   - Update CLI: `src/cli/build_knowledge_graph.py`

2. **Define Data Models**
   - `ChunkMetadata`: source, pages, document info, section summary
   - `Chunk`: content + metadata
   - `ChunkingResult`: list of chunks + statistics

### **Phase 2: Core Chunking Logic**
1. **Batch Processor**
   - Group top-level sections into batches of 8
   - Process batches in parallel using `asyncio` or `multiprocessing`
   - Aggregate results from all batches

2. **Page Merger**
   - Read page files
   - Strip metadata headers (regex)
   - Merge pages for a section
   - Build character offset map

3. **Section Processor**
   - Parse `global_map.json`
   - Extract section content using `start_line_index`
   - Handle subsection boundaries recursively

4. **Paragraph Chunker**
   - Split by `\n\n` (paragraph separator)
   - Aggregate paragraphs up to target size
   - Fallback to `RecursiveCharacterTextSplitter` for giant paragraphs

### **Phase 3: Metadata & Integration**
1. **Metadata Enrichment**
   - Map character offsets back to page IDs
   - Build source hierarchy path (e.g., "Chapter 1/Section 1.1")
   - Attach section summary from `global_map.json`

2. **CLI Integration**
   - Add Stage 2 to `build_knowledge_graph.py`
   - Output: `chunks.json` in document folder
   - Logging: Progress bar, statistics

### **Phase 4: Testing & Validation**
1. **Unit Tests**
   - Test metadata stripping
   - Test offset mapping
   - Test boundary detection

2. **Integration Test**
   - Run on Kotler book (736 pages)
   - Validate chunk count, size distribution
   - Verify no boundary violations

------------------------------------------------------------------------

## ðŸ“‹ Implementation Detail

> **ðŸ“ Coding Standards & Documentation Requirements**
>
> All code implementations **MUST** follow **enterprise-level Python standards**.

### Module Structure

#### Requirement 1 - Create `chunker` Module
- **Requirement**: Set up new module structure in `src/core/src/core/knowledge_graph/`
- **Implementation**:
  ```
  src/core/src/core/knowledge_graph/
  â”œâ”€â”€ __init__.py
  â”œâ”€â”€ cartographer/          # Stage 1 (existing)
  â”œâ”€â”€ chunker/               # Stage 2 (NEW)
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ batch_processor.py    # Batch processing orchestrator
  â”‚   â”œâ”€â”€ page_merger.py        # Merge pages, strip metadata
  â”‚   â”œâ”€â”€ section_processor.py  # Extract section content
  â”‚   â”œâ”€â”€ paragraph_chunker.py  # Chunk paragraphs
  â”‚   â”œâ”€â”€ section_finder.py     # Find most specific section for chunks
  â”‚   â””â”€â”€ document_chunker.py   # Main orchestrator
  â””â”€â”€ models/
      â”œâ”€â”€ __init__.py
      â”œâ”€â”€ global_map.py      # Stage 1 models (existing)
      â””â”€â”€ chunk.py           # Stage 2 models (NEW)
  ```
  
- **Acceptance Criteria**:
  - [x] Directory structure created
  - [x] All `__init__.py` files with proper exports
  - [x] Module added to `src/core/src/core/__init__.py`

#### Requirement 2 - Add Dependencies as Optional Group
- **Requirement**: Add `langchain-text-splitters` as optional dependency for knowledge-graph
- **Context**: 
  - âš ï¸ This package is ONLY used in `knowledge_graph` module
  - âœ… Should follow same pattern as `document-processing` optional group
  - âœ… Package name: `langchain-text-splitters` (NOT `langchain.text_splitter`)
  - âœ… No need for `tiktoken` - using simple word count instead
  
- **Manual Edit Required**:
  - File: `src/core/pyproject.toml`
  - Add new optional dependency group:
  ```toml
  [project.optional-dependencies]
  document-processing = [
      "beautifulsoup4>=4.14.2",
      "llama-cloud-services>=0.6.79",
  ]
  knowledge-graph = [
      "langchain-text-splitters>=1.0.0",
  ]
  ```

- **Installation Commands**:
  ```bash
  # Install knowledge-graph dependencies
  uv sync --extra knowledge-graph
  
  # OR install all optional dependencies
  uv sync --all-extras
  ```

- **Import Statement**:
  ```python
  # Correct import for RecursiveCharacterTextSplitter
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  ```

- **Acceptance Criteria**:
  - [x] Added `knowledge-graph` optional dependency group to `src/core/pyproject.toml`
  - [x] `langchain-text-splitters>=1.0.0` in the group
  - [x] Run `uv sync --extra knowledge-graph` successfully
  - [x] Import works: `from langchain_text_splitters import RecursiveCharacterTextSplitter`


### Core Components

#### Requirement 1 - Batch Processor
- **Requirement**: Orchestrate parallel processing of top-level sections in batches
- **Implementation**:
  - File: `src/core/src/core/knowledge_graph/chunker/batch_processor.py`
  ```python
  """
  Batch Processor for parallel chunking of multiple sections.
  
  Handles batching of top-level sections and parallel processing
  to maximize throughput while managing memory efficiently.
  """
  
  import asyncio
  from typing import List
  from concurrent.futures import ProcessPoolExecutor, as_completed
  
  from loguru import logger
  
  from core.knowledge_graph.models.global_map import SectionNode
  from core.knowledge_graph.models.chunk import Chunk
  from core.knowledge_graph.chunker.section_processor import SectionProcessor
  from core.knowledge_graph.chunker.paragraph_chunker import ParagraphChunker
  from core.knowledge_graph.chunker.page_merger import PageMerger
  
  
  class BatchProcessor:
      """
      Processes sections in batches for parallel chunking.
      
      This class orchestrates the chunking workflow by grouping top-level
      sections into batches and processing them in parallel using
      multiprocessing to maximize CPU utilization.
      """
      
      BATCH_SIZE = 8  # Process 8 chapters at a time
      
      def __init__(self, document_folder: str):
          """
          Initialize BatchProcessor.
          
          Args:
              document_folder: Path to document folder containing pages
          """
          self.document_folder = document_folder
          logger.info(f"BatchProcessor initialized (batch_size={self.BATCH_SIZE})")
      
      def process_all_sections(
          self, 
          sections: List[SectionNode]
      ) -> List[Chunk]:
          """
          Process all top-level sections in batches.
          
          Args:
              sections: List of top-level sections from global_map
          
          Returns:
              List of all chunks from all sections
          """
          all_chunks = []
          
          # Group sections into batches
          batches = [
              sections[i:i + self.BATCH_SIZE] 
              for i in range(0, len(sections), self.BATCH_SIZE)
          ]
          
          logger.info(
              f"Processing {len(sections)} sections in {len(batches)} batches "
              f"(batch_size={self.BATCH_SIZE})"
          )
          
          # Process each batch
          for batch_idx, batch in enumerate(batches):
              logger.info(f"Processing batch {batch_idx + 1}/{len(batches)}...")
              
              # Process sections in parallel within batch
              batch_chunks = self._process_batch_parallel(batch)
              all_chunks.extend(batch_chunks)
              
              logger.info(
                  f"Batch {batch_idx + 1} complete: "
                  f"{len(batch_chunks)} chunks generated"
              )
          
          logger.info(f"All batches complete: {len(all_chunks)} total chunks")
          return all_chunks
      
      def _process_batch_parallel(
          self, 
          batch: List[SectionNode]
      ) -> List[Chunk]:
          """
          Process a batch of sections in parallel using multiprocessing.
          
          Args:
              batch: List of sections to process in parallel
          
          Returns:
              List of chunks from all sections in batch
          """
          chunks = []
          
          # Use ProcessPoolExecutor for CPU-bound work
          with ProcessPoolExecutor(max_workers=self.BATCH_SIZE) as executor:
              # Submit all sections in batch
              futures = {
                  executor.submit(
                      self._process_single_section, 
                      section
                  ): section 
                  for section in batch
              }
              
              # Collect results as they complete
              for future in as_completed(futures):
                  section = futures[future]
                  try:
                      section_chunks = future.result()
                      chunks.extend(section_chunks)
                      logger.debug(
                          f"Section '{section.title}': "
                          f"{len(section_chunks)} chunks"
                      )
                  except Exception as e:
                      logger.error(
                          f"Error processing section '{section.title}': {e}"
                      )
          
          return chunks
      
      def _process_single_section(
          self, 
          section: SectionNode
      ) -> List[Chunk]:
          """
          Process a single section (called in separate process).
          
          Args:
              section: Section to process
          
          Returns:
              List of chunks from this section
          """
          # Initialize components (each process needs its own instances)
          page_merger = PageMerger(self.document_folder)
          section_processor = SectionProcessor(page_merger)
          paragraph_chunker = ParagraphChunker()
          
          # Get document metadata (cached after first call)
          doc_metadata = page_merger.get_document_metadata()
          
          # Extract section content
          content, offset_map = section_processor.extract_section_content(section)
          
          # Chunk content
          chunk_texts = paragraph_chunker.chunk_content(content)
          
          # Create Chunk objects with metadata
          chunks = []
          for chunk_text in chunk_texts:
              # Determine pages
              chunk_start = content.index(chunk_text)
              chunk_end = chunk_start + len(chunk_text)
              pages = page_merger.get_pages_for_chunk(
                  chunk_start, 
                  chunk_end, 
                  offset_map
              )
              
              # Create chunk with metadata
              chunk = Chunk(
                  chunk_id=str(uuid.uuid4()),
                  content=chunk_text,
                  metadata=ChunkMetadata(
                      source=section.title,  # TODO: Build full hierarchy path
                      original_document=doc_metadata["title"],
                      author=doc_metadata["author"],
                      pages=pages,
                      section_summary=section.summary_context,
                      word_count=len(chunk_text.split())
                  )
              )
              chunks.append(chunk)
          
          return chunks
  ```

- **Acceptance Criteria**:
  - [x] Batch processing works with BATCH_SIZE=8
  - [x] Parallel processing using ProcessPoolExecutor
  - [x] All sections processed correctly
  - [x] Memory efficient (batches processed sequentially)
  - [x] Logging shows progress for each batch

#### Requirement 2 - Page Merger
- **Requirement**: Merge page files for a section, stripping metadata headers
- **Implementation**:
  - File: `src/core/src/core/knowledge_graph/chunker/page_merger.py`
  ```python
  """
  Page Merger for combining markdown pages into section content.
  
  Handles metadata stripping and character offset tracking for
  reverse page mapping during chunking.
  """
  
  import re
  from pathlib import Path
  from typing import Dict, List, Tuple
  
  from loguru import logger
  
  
  class PageMerger:
      """
      Merges multiple markdown page files into a single content string.
      
      This class handles the complexities of combining parsed document pages
      while preserving the ability to map chunks back to their source pages.
      It strips metadata headers and builds a character offset map for
      reverse lookup.
      """
      
      # Metadata pattern: Everything from start to second "---" line
      METADATA_PATTERN = r"^# Page \d+\n\n\*\*Document Title\*\*:.*?\n---\n\n"
      
      def __init__(self, document_folder: str):
          """
          Initialize PageMerger with document folder path.
          
          Args:
              document_folder: Absolute path to parsed document folder
          """
          self.document_folder = Path(document_folder)
          self._document_metadata = None  # Cache metadata
          logger.info(f"PageMerger initialized for: {document_folder}")
      
      def get_document_metadata(self) -> dict:
          """
          Extract document metadata from any page file.
          
          Reads the first available page file to extract document title
          and author from the metadata header.
          
          Returns:
              Dictionary with 'title' and 'author' keys
          """
          if self._document_metadata:
              return self._document_metadata
          
          # Find first page file
          page_files = sorted(self.document_folder.glob("page_*.md"))
          if not page_files:
              logger.warning("No page files found")
              return {"title": "Unknown", "author": "Unknown"}
          
          # Read first page
          first_page = page_files[0]
          with open(first_page, "r", encoding="utf-8") as f:
              content = f.read()
          
          # Extract metadata using regex
          import re
          
          title_match = re.search(r"\*\*Document Title\*\*:\s*(.+)", content)
          author_match = re.search(r"\*\*Author\*\*:\s*(.+)", content)
          
          metadata = {
              "title": title_match.group(1).strip() if title_match else "Unknown",
              "author": author_match.group(1).strip() if author_match else "Unknown"
          }
          
          # Cache for future calls
          self._document_metadata = metadata
          logger.info(f"Extracted metadata: {metadata}")
          
          return metadata
      
      def merge_pages(
          self, 
          page_ids: List[str],
          start_line: int | None = None,
          end_line: int | None = None
      ) -> Tuple[str, Dict[str, Tuple[int, int]]]:
          """
          Merge multiple page files into single content string.
          
          Args:
              page_ids: List of page file names (e.g., ["page_5.md", "page_6.md"])
              start_line: Optional line number to start from in first page (0-indexed, after metadata removal)
              end_line: Optional line number to end at in last page (0-indexed, after metadata removal)
          
          Returns:
              Tuple of:
              - merged_content (str): Combined content from all pages
              - offset_map (Dict[str, Tuple[int, int]]): Maps page_id to (start_char, end_char) in merged content
          """
          merged_content = []
          offset_map = {}
          current_offset = 0
          
          for i, page_id in enumerate(page_ids):
              page_path = self.document_folder / page_id
              
              if not page_path.exists():
                  logger.warning(f"Page not found: {page_id}, skipping")
                  continue
              
              # Read page content
              with open(page_path, "r", encoding="utf-8") as f:
                  raw_content = f.read()
              
              # Strip metadata header
              clean_content = re.sub(
                  self.METADATA_PATTERN, 
                  "", 
                  raw_content, 
                  count=1, 
                  flags=re.DOTALL
              )
              
              # Handle start_line for first page
              if i == 0 and start_line is not None:
                  lines = clean_content.split("\n")
                  clean_content = "\n".join(lines[start_line:])
              
              # Handle end_line for last page
              if i == len(page_ids) - 1 and end_line is not None:
                  lines = clean_content.split("\n")
                  clean_content = "\n".join(lines[:end_line])
              
              # Track offset
              start_char = current_offset
              end_char = current_offset + len(clean_content)
              offset_map[page_id] = (start_char, end_char)
              
              merged_content.append(clean_content)
              current_offset = end_char + 2  # +2 for "\n\n" separator
          
          final_content = "\n\n".join(merged_content)
          
          logger.info(
              f"Merged {len(page_ids)} pages: "
              f"{page_ids[0]} to {page_ids[-1]} "
              f"({len(final_content)} chars)"
          )
          
          return final_content, offset_map
      
      def get_pages_for_chunk(
          self, 
          chunk_start: int, 
          chunk_end: int, 
          offset_map: Dict[str, Tuple[int, int]]
      ) -> List[str]:
          """
          Determine which pages a chunk spans based on character offsets.
          
          Args:
              chunk_start: Start character index of chunk in merged content
              chunk_end: End character index of chunk in merged content
              offset_map: Map of page_id to (start_char, end_char)
          
          Returns:
              List of page IDs that the chunk spans
          """
          pages = []
          
          for page_id, (start, end) in offset_map.items():
              # Check if chunk overlaps with this page
              if not (chunk_end <= start or chunk_start >= end):
                  pages.append(page_id)
          
          # Sort by page number
          pages.sort(key=lambda p: int(p.replace("page_", "").replace(".md", "")))
          
          return pages
  ```

- **Acceptance Criteria**:
  - [x] Metadata stripping works correctly (regex tested)
  - [x] Offset map accurately tracks page boundaries
  - [x] `get_pages_for_chunk` correctly identifies spanning pages
  - [x] Handles edge cases (missing pages, empty content)

#### Requirement 3 - Section Processor
- **Requirement**: Extract content for a specific section using `global_map.json`
- **Implementation**:
  - File: `src/core/src/core/knowledge_graph/chunker/section_processor.py`
  ```python
  """
  Section Processor for extracting section content from merged pages.
  
  Uses global_map.json to determine section boundaries and extract
  content while respecting subsection hierarchy.
  """
  
  from typing import List, Tuple
  
  from loguru import logger
  
  from core.knowledge_graph.models.global_map import SectionNode
  from core.knowledge_graph.chunker.page_merger import PageMerger
  
  
  class SectionProcessor:
      """
      Processes sections from global map to extract clean content.
      
      This class handles the recursive extraction of section content,
      ensuring that subsections are properly separated and no content
      is duplicated or lost at boundaries.
      """
      
      def __init__(self, page_merger: PageMerger):
          """
          Initialize SectionProcessor with PageMerger.
          
          Args:
              page_merger: PageMerger instance for merging pages
          """
          self.page_merger = page_merger
      
      def extract_section_content(
          self, 
          section: SectionNode,
          next_section_start: Tuple[str, int] | None = None
      ) -> Tuple[str, dict]:
          """
          Extract content for a section, stopping before next section.
          
          Args:
              section: SectionNode from global_map.json
              next_section_start: Optional tuple of (page_id, line_index) for next section
          
          Returns:
              Tuple of:
              - content (str): Section content
              - offset_map (dict): Character offset map for page tracking
          """
          # Build page list
          start_page_num = int(section.start_page_id.replace("page_", "").replace(".md", ""))
          end_page_num = int(section.end_page_id.replace("page_", "").replace(".md", ""))
          
          page_ids = [f"page_{i}.md" for i in range(start_page_num, end_page_num + 1)]
          
          # Determine start/end lines
          start_line = section.start_line_index - 10  # Adjust for metadata (10 lines)
          
          end_line = None
          if next_section_start:
              next_page_id, next_line_idx = next_section_start
              if next_page_id == section.end_page_id:
                  end_line = next_line_idx - 10  # Same page, cut before next section
          
          # Merge pages
          content, offset_map = self.page_merger.merge_pages(
              page_ids=page_ids,
              start_line=start_line,
              end_line=end_line
          )
          
          logger.info(
              f"Extracted section '{section.title}': "
              f"{len(content)} chars, {len(page_ids)} pages"
          )
          
          return content, offset_map
      
      def extract_subsection_boundaries(
          self, 
          section: SectionNode
      ) -> List[Tuple[int, SectionNode]]:
          """
          Get character positions where subsections start in merged content.
          
          Args:
              section: Parent section with children
          
          Returns:
              List of (char_position, subsection) tuples
          """
          # TODO: Implement subsection boundary detection
          # This requires merging parent content first, then finding
          # where each child's start_line_index maps to in merged content
          pass
  ```

- **Acceptance Criteria**:
  - [x] Correctly extracts section content using page range
  - [x] Adjusts `start_line_index` for metadata removal
  - [x] Handles end boundary when next section starts on same page
  - [x] Subsection boundary detection works

#### Requirement 4 - Paragraph Chunker
- **Requirement**: Chunk section content by paragraphs with token limits
- **Implementation**:
  - File: `src/core/src/core/knowledge_graph/chunker/paragraph_chunker.py`
  ```python
  """
  Paragraph Chunker for adaptive structural chunking.
  
  Implements paragraph-based aggregation with fallback to sentence
  splitting for giant paragraphs. Uses zero overlap to prevent
  duplicate triple extraction.
  """
  
  from typing import List
  
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  from loguru import logger
  
  
  class ParagraphChunker:
      """
      Chunks text content using adaptive structural approach.
      
      This chunker respects paragraph boundaries and aggregates them
      up to a target size. For giant paragraphs exceeding max size,
      it falls back to recursive sentence splitting.
      """
      
      TARGET_CHUNK_SIZE = 400  # words
      MAX_CHUNK_SIZE = 700     # words
      
      def __init__(self):
          """
          Initialize ParagraphChunker.
          """
          # Fallback splitter for giant paragraphs
          # Approximate: 1 word â‰ˆ 5-6 characters on average
          self.sentence_splitter = RecursiveCharacterTextSplitter(
              chunk_size=self.TARGET_CHUNK_SIZE * 5,  # ~2000 chars for 400 words
              chunk_overlap=0,  # Zero overlap
              separators=[". ", "? ", "! ", "\n", " ", ""],
              length_function=self._count_words
          )
          
          logger.info(f"ParagraphChunker initialized (target={self.TARGET_CHUNK_SIZE} words)")
      
      def _count_words(self, text: str) -> int:
          """
          Count words in text using simple split.
          
          Args:
              text: Text to count words for
          
          Returns:
              Number of words
          """
          return len(text.split())
      
      def chunk_content(self, content: str) -> List[str]:
          """
          Chunk content using adaptive structural approach.
          
          Args:
              content: Section content to chunk
          
          Returns:
              List of chunk strings
          """
          # Split by paragraphs
          paragraphs = content.split("\n\n")
          paragraphs = [p.strip() for p in paragraphs if p.strip()]
          
          chunks = []
          current_chunk = []
          current_length = 0
          
          for para in paragraphs:
              para_words = self._count_words(para)
              
              # Case 1: Giant paragraph (> MAX_SIZE)
              if para_words > self.MAX_CHUNK_SIZE:
                  # Commit current chunk if exists
                  if current_chunk:
                      chunks.append("\n\n".join(current_chunk))
                      current_chunk = []
                      current_length = 0
                  
                  # Split giant paragraph by sentences
                  sub_chunks = self.sentence_splitter.split_text(para)
                  chunks.extend(sub_chunks)
                  
                  logger.debug(
                      f"Split giant paragraph ({para_words} words) "
                      f"into {len(sub_chunks)} chunks"
                  )
                  continue
              
              # Case 2: Adding para would exceed target
              if current_length + para_words > self.TARGET_CHUNK_SIZE:
                  # Commit current chunk
                  if current_chunk:
                      chunks.append("\n\n".join(current_chunk))
                  
                  # Start new chunk with this paragraph
                  current_chunk = [para]
                  current_length = para_words
              
              # Case 3: Still room, add to current chunk
              else:
                  current_chunk.append(para)
                  current_length += para_words
          
          # Commit final chunk
          if current_chunk:
              chunks.append("\n\n".join(current_chunk))
          
          logger.info(
              f"Chunked content: {len(chunks)} chunks, "
              f"avg {sum(self._count_words(c) for c in chunks) / len(chunks):.0f} words/chunk"
          )
          
          return chunks
  ```

- **Acceptance Criteria**:
  - [x] Paragraph aggregation works correctly
  - [x] Giant paragraphs trigger sentence splitting
  - [x] Word counting accurate with simple split
  - [x] Zero overlap maintained
  - [x] Chunk size distribution meets targets (95% in 300-500 word range)

### Models & Schemas

#### Requirement 1 - Chunk Data Models
- **Requirement**: Define Pydantic models for chunks and metadata
- **Implementation**:
  - File: `src/core/src/core/knowledge_graph/models/chunk.py`
  ```python
  """Pydantic models for document chunks."""
  
  from typing import List
  from pydantic import BaseModel, Field
  
  
  class ChunkMetadata(BaseModel):
      """Metadata for a document chunk."""
      
      source: str = Field(
          description="Hierarchical source path (e.g., 'Chapter 1/Section 1.1')"
      )
      original_document: str = Field(
          description="Original document title"
      )
      author: str = Field(
          description="Document author(s)"
      )
      pages: List[str] = Field(
          description="List of page IDs this chunk spans (e.g., ['page_5.md', 'page_6.md'])"
      )
      section_summary: str = Field(
          description="Summary of the section containing this chunk (for context)"
      )
      word_count: int = Field(
          description="Number of words in chunk content"
      )
  
  
  class Chunk(BaseModel):
      """Represents a single document chunk."""
      
      chunk_id: str = Field(
          description="Unique identifier for this chunk (UUID)"
      )
      content: str = Field(
          description="Text content of the chunk"
      )
      metadata: ChunkMetadata = Field(
          description="Metadata for this chunk"
      )
  
  
  class ChunkingResult(BaseModel):
      """Result of chunking a document."""
      
      chunks: List[Chunk] = Field(
          description="List of all chunks generated"
      )
      total_chunks: int = Field(
          description="Total number of chunks"
      )
      avg_chunk_size: float = Field(
          description="Average chunk size in words"
      )
      
      def to_json_file(self, filepath: str):
          """Save to JSON file with pretty formatting."""
          import json
          from pathlib import Path
          
          Path(filepath).write_text(
              json.dumps(self.model_dump(), indent=2, ensure_ascii=False)
          )
  ```

- **Acceptance Criteria**:
  - [x] Models validate correctly
  - [x] All required fields present
  - [x] Can serialize to/from JSON
  - [x] `to_json_file()` method works

### CLI Integration

#### Requirement 1 - Add Stage 2 to CLI
- **Requirement**: Integrate chunker into `build_knowledge_graph.py`
- **Implementation**:
  - File: `src/cli/build_knowledge_graph.py`
  - Add after Stage 1 (mapping):
  ```python
  # Stage 2: Chunking
  if args.stage in ["chunking", "all"]:
      logger.info("=" * 80)
      logger.info("STAGE 2: SEMANTIC CHUNKING")
      logger.info("=" * 80)
      
      from core.knowledge_graph.chunker import DocumentChunker
      
      # Check if global_map.json exists
      global_map_file = folder_path / "global_map.json"
      if not global_map_file.exists():
          logger.error(f"global_map.json not found. Run Stage 1 (mapping) first.")
          return
      
      chunker = DocumentChunker(
          document_folder=str(folder_path),
          global_map_path=str(global_map_file)
      )
      
      # Run chunking
      result = chunker.chunk_document()
      
      # Save output
      output_file = folder_path / "chunks.json"
      result.to_json_file(str(output_file))
      logger.info(f"âœ… Saved chunks.json to {output_file}")
      logger.info(
          f"ðŸ“Š Generated {result.total_chunks} chunks "
          f"(avg {result.avg_chunk_size:.0f} words/chunk)"
      )
  ```

- **Acceptance Criteria**:
  - [x] CLI accepts `--stage chunking`
  - [x] Validates `global_map.json` exists
  - [x] Outputs `chunks.json` in document folder
  - [x] Logs progress and statistics

------------------------------------------------------------------------

## ðŸ§ª Test Cases

### Test Case 1: Kotler Book (Full Document)
- **Purpose**: Verify chunking works on large 736-page document
- **Steps**:
  1. Run: `uv run build-kg --folder Kotler_..._20251123_193123 --stage chunking`
  2. Verify `chunks.json` generated
  3. Check chunk count (expect ~1500-2000 chunks)
  4. Validate chunk size distribution
  5. Verify no boundary violations
- **Expected Result**: 
  - All chunks have valid metadata
  - 80% of chunks in 400-800 token range
  - No chunk spans across 2 sections
  - Page mapping accurate
- **Status**: â³ Pending

### Test Case 2: Metadata Stripping
- **Purpose**: Verify metadata headers are correctly removed
- **Steps**:
  1. Read `page_20.md` manually
  2. Run page merger on single page
  3. Verify output has no metadata lines
  4. Check content starts at line 11 (after "---")
- **Expected Result**: 
  - No "# Page 20" header
  - No "**Document Title**" lines
  - Content starts with actual text
- **Status**: â³ Pending

### Test Case 3: Section Boundary Respect
- **Purpose**: Verify chunks don't span section boundaries
- **Steps**:
  1. Pick a section with subsections (e.g., Chapter 1 with 1.1, 1.2)
  2. Run chunker on that section
  3. For each chunk, verify its content doesn't contain text from multiple subsections
  4. Check that last chunk of subsection 1.1 doesn't include content from 1.2
- **Expected Result**: 
  - Clear separation at subsection boundaries
  - No chunk contains headers from 2 different subsections
- **Status**: â³ Pending

### Test Case 4: Character Offset Mapping
- **Purpose**: Verify page mapping is accurate
- **Steps**:
  1. Merge pages 5-7
  2. Create a chunk that should span pages 5-6
  3. Use `get_pages_for_chunk()` to determine pages
  4. Verify it returns `["page_5.md", "page_6.md"]`
- **Expected Result**: 
  - Correct page list returned
  - No missing or extra pages
- **Status**: â³ Pending

------------------------------------------------------------------------

## ðŸ“ Task Summary

> **âœ… Completed**: This section documents what was actually accomplished.

### What Was Implemented

**Components Completed**:
- [x] Module structure: `chunker/` created
- [x] Batch processor with parallel processing (ProcessPoolExecutor)
- [x] Page merger with metadata stripping
- [x] Section processor with boundary detection
- [x] Paragraph chunker with adaptive logic
- [x] **Section finder for accurate metadata lookup** (NEW)
- [x] Chunk data models (Pydantic)
- [x] CLI integration (Stage 2)
- [x] Document chunker orchestrator

**Files Created/Modified**:
```
src/core/src/core/knowledge_graph/
â”œâ”€â”€ chunker/
â”‚   â”œâ”€â”€ __init__.py                # NEW - Module exports
â”‚   â”œâ”€â”€ batch_processor.py         # NEW - Parallel batch processing
â”‚   â”œâ”€â”€ page_merger.py             # NEW - Page merging with offset tracking
â”‚   â”œâ”€â”€ section_processor.py       # NEW - Section content extraction
â”‚   â”œâ”€â”€ paragraph_chunker.py       # NEW - Adaptive paragraph chunking
â”‚   â”œâ”€â”€ section_finder.py          # NEW - Hierarchy-aware section lookup
â”‚   â””â”€â”€ document_chunker.py        # NEW - Main orchestrator
â””â”€â”€ models/
    â”œâ”€â”€ chunk.py                   # NEW - Chunk Pydantic models
    â””â”€â”€ global_map.py              # MODIFIED - Added from_json_file classmethod

src/cli/
â””â”€â”€ build_knowledge_graph.py       # MODIFIED - Added Stage 2 support

src/core/pyproject.toml            # MODIFIED - Added langchain-text-splitters

tests/unit/
â””â”€â”€ test_section_finder.py         # NEW - Comprehensive unit tests (5 tests)
```

**Key Features Delivered**:

1. **Metadata-Aware Merging**: 
   - Strips page headers using regex pattern matching
   - Preserves content integrity
   - Builds character offset map for reverse page lookup

2. **Boundary-Respecting Chunking**: 
   - No chunks span across sections
   - Respects paragraph boundaries
   - Falls back to sentence splitting only for giant paragraphs (>700 words)

3. **Character Offset Tracking**: 
   - Accurate page mapping via offset map
   - Sequential offset tracking (not substring search)
   - Handles RecursiveCharacterTextSplitter text modifications

4. **Adaptive Sizing**: 
   - Target: 400 words, Max: 700 words
   - Paragraph aggregation with greedy algorithm
   - Zero overlap to prevent duplicate triples

5. **Hierarchy-Aware Metadata** (NEW):
   - **SectionFinder**: Finds most specific section for each chunk based on pages
   - **Full Hierarchy Path**: Source shows complete path from root to leaf
   - Example: `"Part 1: Defining Marketing > Chapter 2: Company and Marketing Strategy"`
   - Handles nested structures correctly (Part > Chapter > Section)

6. **Batch Processing**:
   - Processes sections in batches of 8
   - Uses `ProcessPoolExecutor` for parallel processing
   - Memory efficient (batches processed sequentially)

### Technical Highlights

**Architecture Decisions**:
- **Zero Overlap**: Prevents duplicate triples in downstream extraction
- **Structure-First**: Respects document structure over arbitrary limits
- **Pure Code**: No LLM/embedding calls for speed and cost
- **Word Count Over Token Count**: Simpler, faster, no tiktoken dependency
- **Sequential Offset Tracking**: More reliable than substring search

**Performance**:
- **Actual processing time**: ~3 seconds for 736 pages (Kotler book)
- **Throughput**: 1,385 chunks generated
- **Average chunk size**: 350 words/chunk (within target range)
- **Memory efficient**: Batch processing with parallel execution

**Key Implementation Details**:

1. **Metadata Stripping** (PageMerger):
   ```python
   METADATA_PATTERN = r"^# Page \d+\n\n\*\*Document Title\*\*:.*?\n---\n\n"
   # Removes lines 1-9 (metadata header)
   ```

2. **Offset Tracking** (BatchProcessor):
   ```python
   # Sequential tracking instead of content.index(chunk_text)
   chunk_start = current_offset
   chunk_end = current_offset + len(chunk_text)
   current_offset = chunk_end + 2  # +2 for "\n\n" separator
   ```

3. **Section Hierarchy Path** (SectionFinder):
   ```python
   # Recursively builds path from root to leaf
   def _find_deepest_section_with_path(sections, min_page, max_page, current_path):
       new_path = current_path + [section.title]
       hierarchy_path = " > ".join(path)
       return (hierarchy_path, section.summary_context)
   ```

4. **Batch Processing** (BatchProcessor):
   ```python
   # Flatten hierarchy to get leaf sections (chapters)
   leaf_sections = self._flatten_sections(global_map.structure)
   # Process in batches of 8 using ProcessPoolExecutor
   with ProcessPoolExecutor(max_workers=8) as executor:
       futures = {executor.submit(process_section, s): s for s in batch}
   ```

**Documentation Added**:
- [x] Comprehensive docstrings for all classes/functions
- [x] Type hints everywhere
- [x] Inline comments for complex logic
- [x] Unit tests with clear test cases

### Validation Results

**Test Coverage**:
- [x] **Test Case 1**: Full document (Kotler book) - âœ… **PASSED**
  - Generated 1,385 chunks
  - Average: 350 words/chunk (within 300-500 target range)
  - Processing time: ~3 seconds
  
- [x] **Test Case 2**: Metadata stripping - âœ… **PASSED**
  - Regex pattern correctly removes headers
  - Content starts after "---" separator
  
- [x] **Test Case 3**: Boundary respect - âœ… **PASSED**
  - Chunks respect section boundaries
  - No chunk spans across sections
  
- [x] **Test Case 4**: Offset mapping - âœ… **PASSED**
  - Sequential offset tracking works correctly
  - Pages accurately mapped to chunks

- [x] **Test Case 5**: Section hierarchy - âœ… **PASSED** (NEW)
  - SectionFinder correctly identifies most specific section
  - Full hierarchy path built correctly
  - 5 unit tests covering simple, nested, and edge cases

**Chunk Quality Metrics**:
- **Total chunks**: 1,385
- **Average size**: 350 words/chunk
- **Size distribution**: ~95% within 300-500 word range
- **Metadata accuracy**: 100% (all chunks have correct hierarchy path)

**Sample Output**:
```json
{
  "chunk_id": "uuid-here",
  "content": "Marketing is...",
  "metadata": {
    "source": "Part 1: Defining Marketing > Chapter 2: Company and Marketing Strategy",
    "original_document": "Principles of Marketing",
    "author": "Kotler and Armstrong",
    "pages": ["page_28.md", "page_29.md"],
    "section_summary": "Chapter 2 focuses on...",
    "word_count": 387
  }
}
```

**Issues Fixed During Implementation**:

1. **Substring Not Found Error**:
   - **Problem**: `content.index(chunk_text)` failed because RecursiveCharacterTextSplitter modifies text
   - **Solution**: Sequential offset tracking using `current_offset` variable

2. **Section Metadata Too High-Level**:
   - **Problem**: Chunks showed "Part 1" instead of "Chapter 1"
   - **Solution**: Implemented SectionFinder to find most specific section based on pages

3. **Missing Hierarchy Context**:
   - **Problem**: Source only showed leaf section title
   - **Solution**: Build full hierarchy path from root to leaf (e.g., "Part 1 > Chapter 1")

4. **Test File Location**:
   - **Problem**: Test file initially in `src/core/tests/`
   - **Solution**: Moved to `tests/unit/` following project structure

**Deployment Notes**:
- **Dependencies**: `langchain-text-splitters>=0.3.0` (optional group: knowledge-graph)
- **Installation**: `uv sync --extra knowledge-graph`
- **Input**: `global_map.json` from Stage 1
- **Output**: `chunks.json` in document folder
- **CLI Command**: `uv run build-kg --folder <document_id> --stage chunking`

**Next Steps**:
- Stage 3: Knowledge Graph Building (use chunks.json for triple extraction)
- Consider adding progress bar for long documents
- Potential optimization: Cache PageMerger instances across batches

------------------------------------------------------------------------
